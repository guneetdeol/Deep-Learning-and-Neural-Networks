IMPROVING DEEP NEURAL NETWORKS: HYPERPARAMETER TUNING, OPTIMIZATION AND REGULARIZATION

- INITIALIZATION:
    This notebook allows one to:
    - Understand that different initialization methods and their impact on your model performance
    - Implement zero initialization and and see it fails to "break symmetry",
    - Recognize that random initialization "breaks symmetry" and yields more efficient models,
    - Understand that you could use both random initialization and scaling to get even better training performance on your model.

- REGULARIZATION:
    This notebook allows us to:
    - Understand that different regularization methods that could help your model.
    - Implement dropout and see it work on data.
    - Recognize that a model without regularization gives you a better accuracy on the training set but nor necessarily on the test set.
    - Understand that you could use both dropout and regularization on your model.

- GRADIENT CHECK:
    In this notebook :
    - Implement gradient checking from scratch.
    - Understand how to use the difference formula to check your backpropagation implementation.
    - Recognize that your backpropagation algorithm should give you similar results as the ones you got by computing the difference formula.
    - Learn how to identify which parameter's gradient was computed incorrectly.
