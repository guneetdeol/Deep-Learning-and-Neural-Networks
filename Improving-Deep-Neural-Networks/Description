IMPROVING DEEP NEURAL NETWORKS: HYPERPARAMETER TUNING, OPTIMIZATION AND REGULARIZATION

- INITIALIZATION:
    This notebook allows one to:
    - Understand that different initialization methods and their impact on your model performance
    - Implement zero initialization and and see it fails to "break symmetry",
    - Recognize that random initialization "breaks symmetry" and yields more efficient models,
    - Understand that you could use both random initialization and scaling to get even better training performance on your model.
